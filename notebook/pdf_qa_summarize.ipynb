{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb673cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from llama_parse import LlamaParse\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de214eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "API_KEY = os.getenv(\"LLAMA_PARSE_API_KEY\")\n",
    "\n",
    "# Input PDF and output directory\n",
    "pdf_path = \"test1.pdf\"\n",
    "output_dir = \"../data/parsed_md\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, \"test1.md\")\n",
    "\n",
    "# Skip parsing if file already exists\n",
    "if os.path.exists(output_file):\n",
    "    print(f\"Markdown file already exists at {output_file}. Skipping parsing.\")\n",
    "else:\n",
    "    # Initialize parser with support for tables and images\n",
    "    parser = LlamaParse(\n",
    "        api_key=API_KEY,\n",
    "        result_type=\"markdown\",\n",
    "        verbose=True,\n",
    "        parsing_instruction=\"\"\"\n",
    "            Please extract all content including tables, images, lists, and headers \n",
    "            in clean and readable markdown format.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "  \n",
    "        \n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da0d39bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: parsing_instruction is deprecated. Use system_prompt, system_prompt_append or user_prompt instead.\n",
      "Started parsing the file under job_id 5f4a44f0-87e0-41b6-bfe7-f96ec913dceb\n",
      "..."
     ]
    }
   ],
   "source": [
    "  # Parse PDF\n",
    "docs = parser.load_data(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b71eefbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PDF successfully parsed and saved to ../data/parsed_md\\test1.md\n"
     ]
    }
   ],
   "source": [
    "# Save parsed markdown\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, doc in enumerate(docs):\n",
    "        f.write(doc.text)  # Use .text attribute to get string content\n",
    "        f.write(\"\\n\\n---\\n\\n\")  # Add separator between pages/docs\n",
    "\n",
    "print(f\"âœ… PDF successfully parsed and saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7af29f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now split the markdown file into chunks \n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Step 1: Convert LlamaParse docs to LangChain Documents\n",
    "lc_docs = [Document(page_content=doc.text) for doc in docs]\n",
    "\n",
    "## Step 2: Define splitters\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "## Step 3: Split into large \"parent\" chunks (for context)\n",
    "parent_chunks = parent_splitter.split_documents(lc_docs)\n",
    "\n",
    "## child chunks (for retrieval)\n",
    "child_chunk=child_splitter.split_documents(lc_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e84a2c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d604637f-7755-4d47-bc2f-fb025fc234e9',\n",
       " '79be8e51-7d78-46e8-bba7-d40b75cc169b',\n",
       " '0a6ac53e-188b-4e97-ac92-c119e118d0e3',\n",
       " '51838858-48d5-41f8-94e2-1690b8446d87',\n",
       " '6daf79e6-a506-4bdc-b5b1-aa5160555272',\n",
       " '963d904a-1974-4de9-a196-6ec8e2204def',\n",
       " '369fb695-3d5d-4de2-aa27-fc21e648fb13',\n",
       " '51d38437-e496-4fcd-a547-72ea7951f1db',\n",
       " 'dafceb91-8208-42e5-883f-dd524e361959',\n",
       " '790fc6ea-9094-4238-96bb-f07e84fa42c9',\n",
       " '42ba919f-f6ba-4107-b5b7-09d82cb773d1',\n",
       " '77fa63f3-6597-44d1-ab31-f21e53d7bf55',\n",
       " '87ff57c6-1d86-403e-b088-bd365295d50a',\n",
       " 'f01a2903-1717-4719-aa02-27b8b3962ae1',\n",
       " '49a9bec0-390e-4fa1-a87a-90c5018f8f31',\n",
       " '4c5e36e6-644c-4305-9786-e3167ef3d211',\n",
       " '56ed13d4-ca30-4fdb-8676-54acde06afa5',\n",
       " '36963738-64ce-4311-9a85-9a309aed682e',\n",
       " 'f2a46cef-7511-454d-973a-7e41b1749cd4',\n",
       " '693e026b-b040-4b55-8556-492750e5ebe5',\n",
       " '2f477b56-7ce8-451c-b032-bf6afc8ccd4d',\n",
       " '3d6c82a8-97e5-4f6f-8294-1ff0f1b216ae',\n",
       " 'da9acfa6-a03e-4a32-ac69-b850dfa3f1c9',\n",
       " 'fc3ed9b5-df47-4f5f-ad6b-370cee44fd77',\n",
       " '7ab0944f-6483-4ee5-ae10-95876c2f53aa',\n",
       " '3bc558fe-3233-499b-8346-aefc26ecba95',\n",
       " 'd37a858e-559b-4c50-813d-b3589f2c1b68',\n",
       " '38be7f4d-43cc-455d-92f5-37b75264457d',\n",
       " 'ebd4f03d-bf5d-410a-ac62-5339809e84ed',\n",
       " '398a438d-335e-4adf-abd7-e1c1a9fc8b7a',\n",
       " '91bc0a33-e661-40ee-a7b3-3bd0bf973390',\n",
       " '42ff9fc2-01a0-401d-94b0-80c037769db2',\n",
       " '9bc79689-78c3-4d02-8f7e-844b7ded0aa4',\n",
       " '038efc6e-7506-4d1f-94fe-a6dbb4a88cb5',\n",
       " '847b65e0-66e9-452e-a0b5-5d40892d3a62',\n",
       " 'bc2d11b1-6e55-4a07-b945-4d1fd9d5281e',\n",
       " 'dd7eb0af-7913-4072-985a-42176d9ed3f1',\n",
       " '5ae82e66-7062-4325-a754-79b7d1626a4b',\n",
       " '1801dee1-3684-4fde-93e3-6db936598893',\n",
       " '7b990a57-8aa1-45ec-a9a6-c8b3a8a6827d',\n",
       " '8ab16fcc-05b4-4e77-badf-d654aef61498',\n",
       " '3a61ee83-913b-4cb4-bdb6-c279ac0f3bfa',\n",
       " 'b827535f-8ce3-470f-a695-16e31ca78356',\n",
       " '73e2c978-057e-4c06-b77d-2d4b251c8403',\n",
       " 'cfbec6cb-2734-4439-ba15-790c7f0ef3ff',\n",
       " 'd2c22d21-16ae-49b9-9d46-4f49fd50b5ca',\n",
       " 'beb6f78a-642a-401d-bd9b-9be9c4330244',\n",
       " '7a7a3dea-61dc-4c9a-b00f-04b27ddecf5e',\n",
       " 'bec96adf-b0cd-4e8e-b8d0-7a74e105f06f',\n",
       " 'a5219392-b7fb-4a62-bdbc-6699c67c530f',\n",
       " 'a494fd14-abab-4a70-8180-ab07b6f23c14',\n",
       " 'f4a2ad5c-1819-4220-bb8b-f54f63fc5437',\n",
       " '92d10f18-f638-4c3a-9268-2dd6492c7285',\n",
       " '166217aa-a72d-4230-9e21-6e1c082bdfdb',\n",
       " '51e9b176-f7b4-4bd1-a4c1-bb80a45e35bc',\n",
       " '902a2356-2bcc-4156-9fc8-9250a8748bdb',\n",
       " 'be93098a-d722-4faa-9d3a-e10af1cf14b2',\n",
       " '663a0dad-f157-4830-80a9-667203022f29',\n",
       " 'a0f430ab-d442-426c-9c9a-817443ddd7ca',\n",
       " '51171853-c3f3-441e-8f0a-9056d6b8774b',\n",
       " '86bba1fc-3a99-4eff-979a-1b708daceb93',\n",
       " 'a9ec4035-1158-45c4-b824-2ab137560fbe',\n",
       " '57b03b5e-f46b-4472-bd26-d63b2fc6d124',\n",
       " '869065a9-1284-49cf-a0d4-84b0ddcc6fc6',\n",
       " 'f57032df-f964-4192-baa8-749b04a990e2',\n",
       " '74986e1e-b280-439b-93d8-0c18fb6f8e61',\n",
       " 'c50f4212-829e-4a44-b154-131d963fe0ab',\n",
       " '2d51c52c-610f-4595-95bb-245a30ea34af',\n",
       " '2bcdbe9d-01f4-4292-b3cd-4d2936ec04b3',\n",
       " '3fe822f2-4365-41ad-9c13-57fc9ee0bb02',\n",
       " 'a9093d59-dabe-4efb-ab92-74b9eb8d57a0',\n",
       " 'd6555aa8-f25e-4a6c-a119-e5dc80856e83',\n",
       " '62a11e2b-acee-4ba3-aaac-902f52611062',\n",
       " '89394083-8a07-450d-a674-781191dc5efa',\n",
       " '13cc1450-7b91-40db-93e0-80a6cbaa99f7',\n",
       " 'bbe3e36b-c300-4956-bd12-af37fcffcb2a',\n",
       " '6f880e0a-9399-450f-9e22-085574371fe9',\n",
       " 'acbb6f22-bc9d-491c-8680-980fa625dde9',\n",
       " '1a6dbc55-ffac-4abd-a465-25ed955af228',\n",
       " '4cfd6808-16b0-4c27-8091-933b789fa5d5',\n",
       " '3a9729b5-79f8-4b1d-b8b0-dc447b23844a',\n",
       " 'b8f9f2a7-2132-4d46-9f0f-a0bd5e19c7df',\n",
       " '92917a5b-a22b-4641-8406-cabffd18517e',\n",
       " '0c35ba10-e575-402b-a318-001890f3ba71',\n",
       " '3b88a6d2-c676-4ab1-a827-73a6edbcbb82',\n",
       " '60e51df0-fc87-4c19-aaf0-60291bce72ed',\n",
       " '7792f30b-d405-4751-87d5-5118b10cb23b',\n",
       " 'ce1be60b-699f-49e0-ad9a-68f108c60cbe',\n",
       " '737d480f-5d1b-45f4-9cdb-7775dc7a3049',\n",
       " '51f37221-4a9c-4668-966e-d0e49925b49b',\n",
       " '09cd2c21-055b-4331-a7b2-88f128dda0c8',\n",
       " 'aab1f9f1-b6ae-4d43-a2c0-ec9678f0202f',\n",
       " 'd7b5620c-ab2f-438b-9e4d-3b8b76daeabe',\n",
       " '58df98d2-3630-492d-926f-4053990c319c',\n",
       " 'e0f75058-41e1-4b9d-9de4-987e72df6495',\n",
       " '7bf91724-8bfb-45b0-9d38-f6c6fe38ca81',\n",
       " '0607b5e9-c46d-4a19-a6fb-7db1a3c98a6d',\n",
       " 'cfac3ecc-a332-44fa-9136-3ff408c82718',\n",
       " '67ef2c8c-3b24-44e6-ab30-de888261785d',\n",
       " '9f3ada10-dbd2-4c13-8b88-619baba34fb4',\n",
       " 'aab5b57e-f3e9-4d2b-abb4-1d68abb68aac',\n",
       " 'c2854b75-5469-4816-b5eb-dbfc00cafd3f']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "## Step 4: Create vector store\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "persist_directory = \"../data/chroma_db\"\n",
    "os.makedirs(\"../data\", exist_ok=True)\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"child_chunks\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "## Step 5: Add small \"child\" chunks to vector store\n",
    "vector_store.add_documents(child_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d021898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vector store & retriever setup complete.\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryByteStore\n",
    "## Step 6: Set up ParentDocumentRetriever\n",
    "store = InMemoryByteStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vector_store,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter\n",
    ")\n",
    "\n",
    "## Step 7: Add large chunks to retriever\n",
    "retriever.add_documents(parent_chunks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a8b45324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: It is the brain of the system that processes all data and their travel along the bus. For example, in order to execute a program, the CPU will read the first instruction from program memory. This instruction is decoded by the CPU and executed. At the completion of the execution of the instruction, the next instruction is fetched from memory and is executed. This procedure is repeated until the\n"
     ]
    }
   ],
   "source": [
    "result=vector_store.similarity_search(\"What is the main topic of the document?\")\n",
    "print(f\"Result: {result[0].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c857e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "   template=\"\"\"\n",
    "You are an AI assistant that answers questions based on provided document content.\n",
    "\n",
    "Use the following context from the document to answer the question accurately.\n",
    "\n",
    "If the question is unrelated to the context, respond with:\n",
    "\"sorry, I don't know related to this topic\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "summarize_prompt=PromptTemplate(\n",
    "\n",
    "    template=\"\"\"You are an AI assistant that summarizes documents.\n",
    "Use the following context to create a concise summary.  \n",
    "Context:{context}\n",
    "Summary:\"\"\",\n",
    "    input_variables=[\"context\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "91de86f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# initialize gemini model\n",
    "api_key=os.getenv(\"GEIMINI_API_KEY\")\n",
    "model=ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.2,\n",
    "    google_api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b849dce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# Output parser\n",
    "parser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "431611b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "# Retrieve context using question\n",
    "context_retriever = RunnableLambda(\n",
    "    lambda x: {\"context\": retriever.get_relevant_documents(x.get(\"question\", \"\")), **x}\n",
    ")\n",
    "\n",
    "# Format for QA Chain\n",
    "chat_formatter = RunnableLambda(lambda x: {\n",
    "    \"context\": \"\\n\\n\".join([doc.page_content for doc in x[\"context\"]]),\n",
    "    \"question\": x[\"question\"]\n",
    "})\n",
    "\n",
    "# Format for Summarize Chain\n",
    "summarize_formatter = RunnableLambda(lambda x: {\n",
    "    \"context\": \"\\n\\n\".join([doc.page_content for doc in x[\"context\"]])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "485a3bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableSequence\n",
    "\n",
    "\n",
    "\n",
    "# QA Chain\n",
    "chat_chain = RunnableSequence(\n",
    "    context_retriever,\n",
    "    chat_formatter,\n",
    "    prompt,\n",
    "    model,\n",
    "    parser\n",
    ")\n",
    "\n",
    "# Summary Chain\n",
    "summarize_chain = RunnableSequence(\n",
    "    context_retriever,\n",
    "    summarize_formatter,\n",
    "    summarize_prompt,\n",
    "    model,\n",
    "    parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d3d590de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableBranch\n",
    "def is_chat_mode(x):\n",
    "    return x.get(\"mode\", \"\").lower() == \"chat\"\n",
    "\n",
    "def is_summarize_mode(x):\n",
    "    return x.get(\"mode\", \"\").lower() == \"summarize\"\n",
    "\n",
    "rag_mode_chain = RunnableBranch(\n",
    "    (is_chat_mode, chat_chain),\n",
    "    (is_summarize_mode, summarize_chain),\n",
    "    RunnableLambda(lambda x: \"âŒ Invalid mode selected. Choose 'chat' or 'summarize'.\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "89c17bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—£ Chat Response:\n",
      " sorry, I don't know related to this topic\n",
      "\n",
      "ðŸ§¾ Summary:\n",
      " This document primarily discusses the use of microcontrollers (MCUs), particularly STMicroelectronics' products, in various applications.  The automotive industry is highlighted as a major driver of MCU development, demanding high performance and reliability in challenging conditions.  The document then contrasts current and future home applications of MCUs, ranging from consumer electronics to smart home technologies.  Finally, it includes a disclaimer regarding liability, intellectual property, and usage restrictions, along with a list of STMicroelectronics' global offices.\n"
     ]
    }
   ],
   "source": [
    "# âœ… Chat Mode\n",
    "response_chat = rag_mode_chain.invoke({\n",
    "    \"mode\": \"chat\",\n",
    "    \"question\": \"who is binisha?\"\n",
    "})\n",
    "print(\"ðŸ—£ Chat Response:\\n\", response_chat)\n",
    "\n",
    "# âœ… Summarize Mode\n",
    "response_summary = rag_mode_chain.invoke({\n",
    "    \"mode\": \"summarize\",\n",
    "    \"question\": \"Summarize the document\"  # used only to retrieve context\n",
    "})\n",
    "print(\"\\nðŸ§¾ Summary:\\n\", response_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16387068",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "RunnableBranch requires at least two branches",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[122]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m final_chain = \u001b[43mRunnableBranch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mqa\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mqa_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msummarize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msummarize_chain\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m result = final_chain.invoke({\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mwho is the prime minister of nepal\u001b[39m\u001b[33m\"\u001b[39m})\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MachineLearning(ExtraProject)\\Pdf_Parsing\\-AI-Chatbot-for-PDF-Q-A-and-Summarization\\.vaichatbot\\Lib\\site-packages\\langchain_core\\runnables\\branch.py:99\u001b[39m, in \u001b[36mRunnableBranch.__init__\u001b[39m\u001b[34m(self, *branches)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(branches) < \u001b[32m2\u001b[39m:\n\u001b[32m     98\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mRunnableBranch requires at least two branches\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    101\u001b[39m default = branches[-\u001b[32m1\u001b[39m]\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    104\u001b[39m     default,\n\u001b[32m    105\u001b[39m     (Runnable, Callable, Mapping),  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    106\u001b[39m ):\n",
      "\u001b[31mValueError\u001b[39m: RunnableBranch requires at least two branches"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".vaichatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
